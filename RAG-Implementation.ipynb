{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dffc348c-ee14-416c-99d0-a2042ee3e28f",
   "metadata": {},
   "source": [
    "## Key Libraries Deep Dive\n",
    "\n",
    "| Library | Purpose | Why Chosen |\n",
    "| :-- | :-- | :-- |\n",
    "| **langchain_community** | Connectors for data sources | Standardized document handling |\n",
    "| **langchain_text_splitters** | Text processing | Context-aware chunking |\n",
    "| **sentence-transformers** | Semantic embeddings | Open-source SOTA models |\n",
    "| **FAISS** | Vector storage | Optimized similarity search |\n",
    "| **langchain_core** | Pipeline construction | Modular architecture |\n",
    "| **ChatPerplexity** | LLM interface | Commercial-grade performance |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55562ca7-adaf-481d-b6b7-e708200cc95a",
   "metadata": {},
   "source": [
    "# Workflow Summary\n",
    "1. **Ingest** content from web sources\n",
    "2. **Process** documents into manageable chunks\n",
    "3. **Encode** text into numerical representations\n",
    "4. **Store** vectors for efficient retrieval\n",
    "5. **Retrieve** relevant context for queries\n",
    "6. **Generate** answers using LLM with injected context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49ed185-d6cf-4178-a65d-88b024c3a877",
   "metadata": {},
   "source": [
    "# Environment Setup\n",
    "- **os**: Manages environment variables for secure credential handling\n",
    "- **Security Note**: Always keep API keys in secure storage (never hardcode in production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09a20301-3542-41a0-8b4a-da89dac66bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"XXXXXXX\"  # Authentication for API services - Fill the api key\n",
    "os.environ[\"USER_AGENT\"] = \"Learning RAG\"  # Identifies requests to web servers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec6d7e2-9231-4c18-bda3-072e17a4b6a3",
   "metadata": {},
   "source": [
    "# Document Ingestion\n",
    "- **WebBaseLoader**: Specialized web scraper that preserves metadata\n",
    "- **Alternative loaders**: Available for PDFs, CSVs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bf7a1af-d63d-4feb-810d-3339ae39b0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "urls = [\"https://en.wikipedia.org/wiki/Retrieval-augmented_generation\"]\n",
    "loader = WebBaseLoader(urls)  # Web content fetcher\n",
    "documents = loader.load()  # Returns list of Document objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe885ba-678c-4ae8-a529-9217cc0e342d",
   "metadata": {},
   "source": [
    "# Text Chunking\n",
    "- **RecursiveCharacterTextSplitter**: Maintains semantic structure better than simple splitting\n",
    "- Chunk size affects retrieval quality - larger chunks capture more context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3efe7b3e-0759-444a-9ded-99f05b820b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # Optimal for balance between context and precision\n",
    "    chunk_overlap=200  # Maintains context continuity between chunks\n",
    ")\n",
    "splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4856ece7-0c8c-4d19-9ebb-fa23d71cad15",
   "metadata": {},
   "source": [
    "# Vector Embedding & Storage\n",
    "- **HuggingFaceEmbeddings**: Converts text to numerical representations\n",
    "- **FAISS**: Facebook's library for fast similarity searches (lower memory footprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c8198e1-7fa3-45b0-abc3-738a91fc834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"  # State-of-the-art sentence embeddings\n",
    ")\n",
    "vectorstore = FAISS.from_documents(splits, embedding)  # Efficient similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d3e1b1-21c5-485f-9aa6-2a15a5175a6a",
   "metadata": {},
   "source": [
    "# RAG Chain Construction\n",
    "- **ChatPromptTemplate**: Manages LLM instruction formatting\n",
    "- **RunnablePassthrough**: Directly pipes user questions to prompt\n",
    "- **Temperature** 0.7 allows creative but focused responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab3ef795-551f-4a69-b002-8c2bef13e706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_perplexity import ChatPerplexity\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"Answer using ONLY these context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)  # Structured prompt engineering\n",
    "\n",
    "retriever = vectorstore.as_retriever()  # Creates search interface\n",
    "\n",
    "llm = ChatPerplexity(\n",
    "    model=\"sonar-pro\",  # High-performance commercial LLM\n",
    "    temperature=0.7  # Balances creativity vs factual accuracy\n",
    ")\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4433d5cd-47eb-4a65-bf30-af9618006fc7",
   "metadata": {},
   "source": [
    "# Query Execution\n",
    "- Invocation pattern matches LangChain's standard interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66fa9edb-6487-4f4e-9a21-7afb9e3a3dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## RAG Architecture Explained\n",
      "\n",
      "Retrieval-Augmented Generation (RAG) is an architecture that enhances large language models (LLMs) by integrating an information retrieval component before generating responses. This design allows LLMs to access up-to-date, relevant information from external sources, improving factual accuracy and reducing errors known as hallucinations[4][5][cb86e46a-1831-467e-9092-75d961710bd5].\n",
      "\n",
      "**Key Components and Stages**\n",
      "\n",
      "- **Indexing**  \n",
      "  Data (usually unstructured text, but also semi-structured or structured data) is first processed and converted into numerical representations called embeddings. These embeddings map content into a large vector space, capturing semantic meaning. The embeddings are stored in a vector database, allowing efficient retrieval of relevant information. The data is typically split into smaller chunks (such as sentences or paragraphs) before embedding to improve retrieval granularity[3][5][815dbf86-882d-426f-bc6a-b45411acec55].\n",
      "\n",
      "- **Retrieval**  \n",
      "  When a user submits a query, a retriever module processes the query—often embedding it into the same vector space—and searches the database for the most relevant chunks. Retrieval methods can include similarity searches using dense or sparse vectors and may use advanced techniques like approximate nearest neighbor (ANN) searches or hybrid approaches that mix dense and sparse vectors for improved accuracy and efficiency[3][4][5][815dbf86-882d-426f-bc6a-b45411acec55].\n",
      "\n",
      "- **Generation**  \n",
      "  The selected relevant documents are combined with the user's query and provided as context to the LLM. The LLM then generates a response grounded in this retrieved information. This process helps the model produce answers that are more accurate, up-to-date, and verifiable[cb86e46a-1831-467e-9092-75d961710bd5][99b611ea-076c-4796-afdb-bb50e7fc2185].\n",
      "\n",
      "**Improvements and Variants**\n",
      "\n",
      "- The retrieval process can be optimized through encoder improvements (e.g., better vector representations), retriever-centric training (e.g., supervised retriever optimization, reranking), and hybrid retrieval techniques[4].\n",
      "- The RAG process can be further adapted by redesigning the language model to work more efficiently with the retriever, as in the Retro and Retro++ variants, which can achieve similar performance with much smaller models[1c1b36ee-62ca-498b-9890-4cb47b410f93].\n",
      "\n",
      "**Benefits**\n",
      "\n",
      "- **Accuracy**: By grounding responses in retrieved documents, RAG helps reduce hallucinations and increases factual correctness[cb86e46a-1831-467e-9092-75d961710bd5].\n",
      "- **Transparency**: RAG can include source citations, enabling users to verify information[99b611ea-076c-4796-afdb-bb50e7fc2185].\n",
      "- **Efficiency**: It reduces the need to frequently retrain LLMs on new data, saving computational resources[99b611ea-076c-4796-afdb-bb50e7fc2185].\n",
      "\n",
      "## Summary Table: RAG Architecture\n",
      "\n",
      "| Stage     | Function                                                                                 |\n",
      "|-----------|-----------------------------------------------------------------------------------------|\n",
      "| Indexing  | Convert documents to embeddings; store in a vector database for efficient retrieval      |\n",
      "| Retrieval | Use a retriever to find the most relevant chunks based on similarity to the user query   |\n",
      "| Generation| LLM generates a response using retrieved content as context, improving accuracy and trust|\n",
      "\n",
      "RAG thus represents a powerful approach to make LLMs more reliable, current, and transparent by combining the strengths of retrieval systems and generative models[cb86e46a-1831-467e-9092-75d961710bd5][99b611ea-076c-4796-afdb-bb50e7fc2185][815dbf86-882d-426f-bc6a-b45411acec55].\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"Explain RAG architecture\")\n",
    "print(response.content)  # Displays formatted answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
