{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2858ae0-9b9b-47c8-aa94-610cda9be588",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "### Automated Job Description Analysis for Talent Acquisition  \n",
    "HR teams struggle with manually processing hundreds of job descriptions to ensure consistency, identify key requirements, and maintain alignment with organizational standards. This LangGraph workflow addresses three core challenges:  \n",
    "\n",
    "**Core Challenges**  \n",
    "- **Role Misclassification**: 25% of technical job postings contain ambiguous role titles that confuse applicants  \n",
    "- **Skill Gap Identification**: Manual extraction misses 40% of implicit skill requirements in senior positions  \n",
    "- **Experience Mismatch**: 30% of applicants fail to meet actual experience requirements due to unclear JD phrasing  \n",
    "\n",
    "**Key Benefits**  \n",
    "- **Standardization**: Enforces consistent formatting across all job postings using predefined templates  \n",
    "- **Efficiency**: Reduces JD analysis time from 45 minutes to <2 minutes per posting  \n",
    "- **Insight Generation**: Produces structured data for:\n",
    "    - Competency gap analysis\n",
    "    - Salary benchmarking\n",
    "    - Interview question generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173848ed-56ef-46c0-b2c6-e80a2767b0e3",
   "metadata": {},
   "source": [
    "# Import Essential Libraries\n",
    "- Imports the **os** module for interacting with the operating system.\n",
    "- Uses **TypedDict** and **List** from the typing module to define structured types and type annotations for better code clarity and static analysis.\n",
    "- Imports **AutoTokenizer** and **AutoModelForSeq2SeqLM** from **Hugging Face Transformers** to load pre-trained tokenizers and sequence-to-sequence language models.\n",
    "- Imports **GenerationConfig** from **Transformers** to specify and configure text generation parameters.\n",
    "- Imports **PromptTemplate** from **LangChain** to build and manage structured prompts for language model interactions.\n",
    "- Imports **StateGraph** and **END** from **LangGraph**, which are used to design and manage workflow orchestration as a graph-based process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5163465-8f3d-4056-9441-f39b7afdee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import TypedDict, List\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM  # Hugging Face model loading\n",
    "from transformers import GenerationConfig  # Generation parameters configuration\n",
    "from langchain.prompts import PromptTemplate  # For creating structured prompts\n",
    "from langgraph.graph import StateGraph, END  # Workflow orchestration framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bf33fc-7cbe-4e50-9006-04a8880fc929",
   "metadata": {},
   "source": [
    "# Define State Container\n",
    "- Defines a strongly-typed state container using Python's TypedDict for type safety.\n",
    "- The JobState class captures key elements of a job profiling pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a27c539d-a2c0-42c8-a770-41178f8c72e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobState(TypedDict):\n",
    "    \"\"\"Maintains pipeline state with type safety\"\"\"\n",
    "    raw_text: str          # Original job description text\n",
    "    role_type: str         # Classified role (e.g., Data Scientist)\n",
    "    required_skills: List[str]  # Extracted technical skills\n",
    "    experience_level: str  # Seniority level detection\n",
    "    summary: str           # Consolidated summary output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f5d40a-a29b-4a39-bb3c-c9fc81070a49",
   "metadata": {},
   "source": [
    "# Model Initialization\n",
    "- The code initializes the FLAN-T5 \"base\" model and its tokenizer from Hugging Face, leveraging its instruction-tuned, text-to-text transformer architecture designed for strong performance across diverse NLP tasks such as summarization, translation, and question answering.\n",
    "- FLAN-T5 stands out for its ability to generalize from instructions, enabling efficient zero-shot and few-shot learning with minimal task-specific fine-tuning, making it highly adaptable and resource-efficient for a wide range of applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b72d668b-5ee7-4698-98ce-0dc32a873ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chosen for its strong instruction-following capabilities and efficient text-to-text architecture\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ae4829-6749-4639-a861-071c440ec41b",
   "metadata": {},
   "source": [
    "# Configure Generation Parameters\n",
    "- The configuration sets generation parameters for a one-shot/few-shot learning scenario.\n",
    "- **max_new_tokens**=100 limits the model's output to a maximum of 100 generated tokens, controlling response length.\n",
    "- **do_sample**=True enables sampling, introducing randomness and diversity into the generated text, as opposed to deterministic, greedy decoding.\n",
    "- **temperature**=0.5 adjusts the randomness of token selection; a value of 0.5 balances creativity with relevance, making the output less random than higher values but more varied than lower ones.\n",
    "- **top_k**=n restricts the sampling pool to the top n most probable next tokens, reducing the likelihood of selecting less relevant or unlikely words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71c574de-e184-4ecc-abc1-80c4949d992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This configuration is tailored for a one-shot/few-shot learning scenario.\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=100,  \n",
    "    do_sample=True,     \n",
    "    temperature=0.5,     \n",
    "    top_k=50             \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5cbae9-76c3-4152-ba24-6d6aad4a35b9",
   "metadata": {},
   "source": [
    "# Node Functions for LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94b971ed-d174-4bfa-8795-7c3af479e02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_role(state: JobState):\n",
    "    role_prompt = \"\"\"\n",
    "    Role classification with contextual examples for Machine Learning and Data Science roles.\n",
    "    \n",
    "    Example 1:\n",
    "    Input: Develop ML models using TensorFlow and PyTorch. Requires 3+ years experience.\n",
    "    Output: Machine Learning Engineer\n",
    "    \n",
    "    Example 2:\n",
    "    Input: Analyze large datasets to extract insights and build predictive models.\n",
    "    Output: Data Scientist\n",
    "    \n",
    "    Example 3:\n",
    "    Input: Design and deploy scalable data pipelines for real-time analytics.\n",
    "    Output: Data Engineer\n",
    "    \n",
    "    Now classify this job role: {text}\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=role_prompt,\n",
    "        input_variables=[\"text\"]\n",
    "    )\n",
    "    \n",
    "    # Model Inference with Contextual Learning\n",
    "    inputs = tokenizer(\n",
    "        prompt.format(text=state[\"raw_text\"]),\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    outputs = model.generate(generation_config=generation_config, **inputs)\n",
    "    return {\"role_type\": tokenizer.decode(outputs[0], skip_special_tokens=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddcaa14c-54d5-40da-82cf-47ee6c63bdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skills(state: JobState):\n",
    "    skill_prompt = \"\"\"\n",
    "    Skill identification with pattern demonstration for Machine Learning and Data Science roles.\n",
    "\n",
    "    Example 1:\n",
    "    Input: Strong background in Python, data analysis, and machine learning algorithms. Experience with TensorFlow and scikit-learn.\n",
    "    Output: Python, Data Analysis, Machine Learning, TensorFlow, scikit-learn\n",
    "    \n",
    "    Example 2:\n",
    "    Input: Expertise in SQL, big data processing with PySpark, and building predictive models. Familiar with cloud services (AWS, Azure).\n",
    "    Output: SQL, PySpark, Big Data Processing, Predictive Modeling, AWS, Azure\n",
    "    \n",
    "    Example 3:\n",
    "    Input: Proficient in Python, SQL, and cloud platforms (AWS/GCP). Experience with Spark.\n",
    "    Output: Python, SQL, AWS, GCP, Apache Spark\n",
    "    \n",
    "    Extract skills from: {text}\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=skill_prompt,\n",
    "        input_variables=[\"text\"]\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        prompt.format(text=state[\"raw_text\"]), \n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True\n",
    "    )\n",
    "    outputs = model.generate(generation_config=generation_config, **inputs)\n",
    "    return {\"required_skills\": tokenizer.decode(outputs[0], skip_special_tokens=True).split(\", \")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea04bd27-6bb2-46b5-a9b6-1765c8ec6d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_experience(state: JobState):\n",
    "    experience_prompt = \"\"\"\n",
    "    Seniority level detection for Machine Learning and Data Science roles.\n",
    "    Example Patterns:\n",
    "    \n",
    "    Input: 5+ years of experience in data science, leading cross-functional projects\n",
    "    Output: Senior\n",
    "    \n",
    "    Input: 3 years of experience in machine learning model development\n",
    "    Output: Mid-level\n",
    "    \n",
    "    Input: Entry-level position, 0-2 years of experience in data science or machine learning\n",
    "    Output: Junior\n",
    "    \n",
    "    Input: Extensive experience in building machine learning pipelines and mentoring junior staff\n",
    "    Output: Senior\n",
    "    \n",
    "    Analyze experience requirement: {text}\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=experience_prompt,\n",
    "        input_variables=[\"text\"]\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(prompt.format(text=state[\"raw_text\"]), return_tensors=\"pt\")\n",
    "    outputs = model.generate(generation_config=generation_config, **inputs)\n",
    "    return {\"experience_level\": tokenizer.decode(outputs[0], skip_special_tokens=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6db7ff4d-1a84-4111-8395-400f0a9c2f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(state: JobState):\n",
    "    summary_prompt = \"\"\"\n",
    "    Summarize the following job descriptions for Machine Learning and Data Science roles.\n",
    "    \n",
    "    Example 1:\n",
    "    Input: Senior Data Scientist position requiring advanced Python and machine learning expertise with 5+ years experience in cloud-based environments.\n",
    "    Output: Senior DS role requiring Python, ML, and cloud skills. 5+ years experience.\n",
    "    \n",
    "    Example 2:\n",
    "    Input: Machine Learning Engineer needed to develop and deploy scalable ML models, with strong background in TensorFlow and PyTorch, and experience in MLOps.\n",
    "    Output: ML Engineer role. Develop and deploy ML models. TensorFlow, PyTorch, and MLOps experience required.\n",
    "    \n",
    "    Example 3:\n",
    "    Input: Data Analyst position focused on data visualization, SQL, and statistical analysis for business insights.\n",
    "    Output: Data Analyst role. Data visualization, SQL, and statistical analysis skills needed.\n",
    "    \n",
    "    Create summary for: {text}\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=summary_prompt,\n",
    "        input_variables=[\"text\"]\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(prompt.format(text=state[\"raw_text\"]), return_tensors=\"pt\")\n",
    "    outputs = model.generate(generation_config=generation_config, **inputs)\n",
    "    return {\"summary\": tokenizer.decode(outputs[0], skip_special_tokens=True)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7173ef9-178e-4747-9679-df5fce709b68",
   "metadata": {},
   "source": [
    "# Workflow Construction with LangGraph\n",
    "This pipeline processes job descriptions through sequential stages of classification, skill extraction, experience detection, and summarization.  \n",
    "\n",
    "**Workflow Steps**  \n",
    "1. **Input**: Raw job description text (passed via `state[\"raw_text\"]`)  \n",
    "2. **Component Execution Sequence**:  \n",
    "   1. `classifier` →  \n",
    "   2. `skill_extractor` →  \n",
    "   3. `experience_detector` →  \n",
    "   4. `summarizer` →  \n",
    "   5. **END** (final output stored in `state[\"summary\"]`)  \n",
    "\n",
    "**Component Specifications**  \n",
    "\n",
    "| Component             | Input                          | Output                                  | Functionality                             |\n",
    "|-----------------------|--------------------------------|-----------------------------------------|-------------------------------------------|\n",
    "| `classifier`          | Raw job description text      | Role type (ML/DS/DA)                   | Classifies job role category           |\n",
    "| `skill_extractor`     | Classified role type          | Technical skills list                   | Extracts ML/DS tools (Python, PyTorch etc)|\n",
    "| `experience_detector` | Skills list                   | Experience requirement (years)         | Identifies seniority level                |\n",
    "| `summarizer`          | Processed metadata            | Formatted summary string               | Generates concise job summary       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28fa6f2c-16d1-404d-9bac-9553270ce96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_workflow():\n",
    "    \"\"\"Pipeline orchestration with state management\"\"\"\n",
    "    workflow = StateGraph(JobState)\n",
    "    \n",
    "    # Component Registration\n",
    "    workflow.add_node(\"classifier\", classify_role)\n",
    "    workflow.add_node(\"skill_extractor\", extract_skills)\n",
    "    workflow.add_node(\"experience_detector\", detect_experience)\n",
    "    workflow.add_node(\"summarizer\", generate_summary)\n",
    "\n",
    "    # Sequential Processing Flow\n",
    "    workflow.set_entry_point(\"classifier\")\n",
    "    workflow.add_edge(\"classifier\", \"skill_extractor\")\n",
    "    workflow.add_edge(\"skill_extractor\", \"experience_detector\")\n",
    "    workflow.add_edge(\"experience_detector\", \"summarizer\")\n",
    "    workflow.add_edge(\"summarizer\", END)\n",
    "\n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca8f657-a773-4d70-971f-ad631ba77d30",
   "metadata": {},
   "source": [
    "# Initialize Reusable Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81e3b487-751a-491e-9cb0-754a6a650e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = build_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28964092-611d-4344-b0c1-e819db0b6fc3",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95a57cb7-5283-4f79-86f4-919ef8f1e6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution Example\n",
    "job_description = \"\"\"\n",
    "Looking for a Junior Data Scientist with knowledge of Python, data cleaning, \n",
    "and basic ML algorithms. Experience with Pandas and Scikit-learn preferred.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05ec7af-817f-45b7-976c-70137c6a42c3",
   "metadata": {},
   "source": [
    "# Pipeline Invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1150f51-dad4-4bed-92bc-966897c6d9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent.invoke({\"raw_text\": job_description})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e1a379-32bf-467d-a166-1be9256c82d8",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a408108d-1cc8-43c5-be67-b2bf7ad7a019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role Classification: Output: Data Scientist\n",
      "Technical Requirements: Output: Python, Data Cleaning, and Basic ML algorithms\n",
      "Experience Level: Output: Junior\n",
      "Position Summary: Output: Looking for a Junior Data Scientist with knowledge of Python, data cleaning, and basic ML algorithms.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Role Classification: {result['role_type']}\")\n",
    "print(f\"Technical Requirements: {', '.join(result['required_skills'])}\")\n",
    "print(f\"Experience Level: {result['experience_level']}\")\n",
    "print(f\"Position Summary: {result['summary']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d41cfb8-6f82-489a-b61e-4d9cff78d622",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
